<html>
<title> EYERON MEYEDEN. Computer Vision. </title>
<head>
	<link rel="stylesheet" type="text/css" href="estilo-general.css" />
	<link rel="stylesheet" type="text/css" href="estilo_info.css" />
	<link href="https://fonts.googleapis.com/css?family=Rubik:500" rel="stylesheet"> 
	<link href="https://fonts.googleapis.com/css?family=Roboto:300" rel="stylesheet">
	
	<link rel="icon" href="web_imagenes/logo.png" type="image/png" sizes="any">

	<h2>
		<a href="portada.html"><img id="logo" src="web_imagenes/nombre+logo_blanco.png" >
		</a>
	</h2>

	<script type="text/javascript">
		function copiarMail() {
			document.getElementById("mail").hidden=false;
			var copyText = document.getElementById("mail");
			copyText.select();
			document.execCommand("Copy");
			document.getElementById("mail").hidden=true;
			alert("Copiado al portapeles: " + copyText.value);
		}
	</script>
</head>

<body>
	
	<div id="menulateral">
	</br>
	<table>
		<tr>
			<td><img border="0" src="web_imagenes/logo_home.png" width="40"></td>
			<td>
				<form action="portada.html">
					<input type="submit" value="Home">
				</form>
			</td>
		</tr>
		<tr>
			<td><img border="0" src="web_imagenes/logo_work.png" width="40"></td>
			<td>
				<form action="info.html">
					<input type="submit" value="Our work">
				</form>
			</td>
		</tr>
		<tr>
			<td><img border="0" src="web_imagenes/logo_nosotros.png" width="40"></td>
			<td>
				<form action="nosotros.html">
					<input type="submit" value="About us">
				</form>
			</td>
		</tr>
	</table>
	</div>

<div id="contenido">
	<div class="texto" align="justify">
		<h2 id="titulo" align="left">How it works</h2>
		Our algorithm is based on the detection of facial <b>Action Units</b> (AUs) to identify happiness, or the lack of it. Action Units describe feature movements of specific parts of the face, like raising the eyebrows or closing the eyes.<br><br>
		But, how can this algorithm detect AUs based on movement if we work with still pictures? Easy: we use a picture where the person has a neutral expression and compare it to the happy (or not) picture. That way we are able to detect "movements" without requiring a video.<br><br>
		After identifying the AUs present in the image, our trained classifier determines if the person in the image shows happiness or not, which is the goal of this project.<br><br>


	</div>

	<div class="texto" align="justify">
		<h2 id="titulo" align="right">Algorithm</h2>
		So, in a deeper level, how does our program decide if a person is happy or not?<br><br>
		The first step is detecting the face in the picture: the contour, the eyes, nose and lips, and translate it to a set of points we can identify as such. This step is done by a trained algorithm developed by another researchers. We just select the points that are interesting for us, which are defined in the following image:<br><br>
		<img border="0" src="web_imagenes/fid_points.png" class="fotocentro"><br><br>
		After identifying these points (called fiducial points) in both the target image and the neutral one, the algorithm normalizes the distances between points in relation to the distance between the inner eye points. With the normalized distances, the next step is comparing both pictures in order to find AUs. This step ends when each AU has a value from 0 to 1 depending on how intense that AU is in the analysed picture. This process is shown on the flowchar below<br><br>
		<img border="0" src="web_imagenes/flujogramaweb.png" class="fotocentro"><br><br>
		The final step is running a classifier with the values of each AU. We tried different classifiers both linear and non-linear. They take the value of all the AUs and decide if the image shows happiness; after that, we compare it to the real value of happiness, provided by the selected database, and asign a ratio of success to each classifier.
		<br><br>
		Furthermore, we have developed a user-friendly program that executes all these steps with the pictures of your choosing. The whole algorithm and user interface can be found in our 
		<a href="https://github.com/JLBicho/VisionPorComputador/"> GitHub repository</a> 
		, open for everyone to download.
	</div>

	<div class="texto" align="justify">
		<h2 id="titulo" align="left">Results</h2>
		The validation of the algorithm was conducted in the next manner; first, the training, then validation and finally results. We trained different classifiers with different training and validation batch sizes. This means that, from the 100% of the original database of images, we selected a percentage to train and the rest to validate the training, to avoid overfitting. We created three models; one used 80% of the original images to train the classifiers, ant the 20% left to validate; we called it Model(80-20). We also had the models Model(60-40) and Model(40-60), which follow the same naming convention.<br/><br/>
		Once the classifiers were trained, we had to put them to work. For this, we selected three databases; the JFFE which we had been using, a database from Caltech, composed of images taken to memebers of the center; and a small database we created with our own faces. To be able to determine the best classifier, we had to compare between all the models and the databases; so we made graphs.<br/><br/>
		<img border="0" src="web_imagenes/all_chart.png" class="fotograph"><br/><br/>
		As the graph shows, the best results are obtained in the database JFFE, which is expected because it is the one we used to train and validate, thus, the images are "known". The next one in precision is our own database, probably due to our knowledge of our own algorithm, making us biased about the facial expressions we made. Finally, the Caltech database is the lowest because the images are not prepared to be used in this manner, the faces of the people is not centered, ilumination is heterogeneous, and many other factors, induce some error in our models. There is no clear winner between models, it was expected that the best one would be the Model(60-40), due to the low risk of overfitting, but the use of more training images. However, the results show that each classfier in each database offers different precisions which do not correlate directly to a specific model. Talking about classifers, the best one in an overall performance is the <i>pasivoAgresivoClassifier</i>. It maintains a good precision in  every model, across all databases. The only cases in which another model is better occurs in the JFFE database, which does not defines a better performance because the images are known. 
	</div>

	<div class="texto" align="justify">
		<h2 id="titulo" align="right">App demonstration</h2>
		<iframe width="560" height="315" src="https://www.youtube.com/embed/xdG5ghY3vTQ" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
	</div>
</div>

	<div id="redes">
		<a  href="https://twitter.com/IronMaiden" target="_blank">
			<img border="0" id="icon" src="web_imagenes/logo_twitter.png" width="50" height="50">
		</a>
	</br></br>
	<a  href="https://ironmaiden.com/" target="_blank">
		<img border="0" id="icon" src="web_imagenes/logo_fb.png" width="50" height="50">
	</a>
	</br></br>
	<a  href="https://www.instagram.com/ironmaiden/" target="_blank">
		<img border="0" id="icon" src="web_imagenes/logo_instagram.png" width="50" height="50">
	</a>
	</br></br>
	<img border="0" id="icon" src="web_imagenes/logo_mail.png" width="50" height="50" onclick="copiarMail()">
	<input type="text" value="ironmaiden@gmail.com" id="mail" hidden>
	</div>
</body>
</html>