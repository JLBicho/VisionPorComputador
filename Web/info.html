<html>
<title> EYERON MEYEDEN. Computer Vision. </title>
<head>
	<link rel="stylesheet" type="text/css" href="estilo-general.css" />
	<link rel="stylesheet" type="text/css" href="estilo_info.css" />
	<link href="https://fonts.googleapis.com/css?family=Rubik:500" rel="stylesheet"> 
	<link href="https://fonts.googleapis.com/css?family=Roboto:300" rel="stylesheet">
	
	<link rel="icon" href="web_imagenes/logo.png" type="image/png" sizes="any">

	<h2>
		<a href="portada.html"><img id="logo" src="web_imagenes/nombre+logo_blanco.png" >
		</a>
	</h2>

	<script type="text/javascript">
		function copiarMail() {
			document.getElementById("mail").hidden=false;
			var copyText = document.getElementById("mail");
			copyText.select();
			document.execCommand("Copy");
			document.getElementById("mail").hidden=true;
			alert("Copiado al portapeles: " + copyText.value);
		}
	</script>
</head>

<body>
	
	<div id="menulateral">
	</br>
	<table>
		<tr>
			<td><img border="0" src="web_imagenes/logo_home.png" width="40"></td>
			<td>
				<form action="portada.html">
					<input type="submit" value="Home">
				</form>
			</td>
		</tr>
		<tr>
			<td><img border="0" src="web_imagenes/logo_work.png" width="40"></td>
			<td>
				<form action="info.html">
					<input type="submit" value="Our work">
				</form>
			</td>
		</tr>
		<tr>
			<td><img border="0" src="web_imagenes/logo_nosotros.png" width="40"></td>
			<td>
				<form action="nosotros.html">
					<input type="submit" value="About us">
				</form>
			</td>
		</tr>
	</table>
	</div>

<div id="contenido">
	<div class="texto" align="justify">
		<h2 id="titulo" align="left">How it works</h2>
		Our algorithm is based on the detection of facial <b>Action Units</b> (AUs) to identify happiness, or the lack of it. Action Units describe feature movements of specific parts of the face, like raising the eyebrows or closing the eyes.<br><br>
		But, how can this algorithm detect AUs based on movement if we work with still pictures? Easy: we use a picture where the person has a neutral expression and compare it to the happy (or not) picture. That way we are able to detect "movements" without requiring a video.<br><br>
		After identifying the AUs present in the image, our trained classifier determines if the person in the image shows happiness or not, which is the goal of this project.<br><br>


	</div>

	<div class="texto" align="justify">
		<h2 id="titulo" align="right">Algorithm</h2>
		So, in a deeper level, how does our program decide if a person is happy or not?<br><br>
		The first step is detecting the face in the picture: the contour, the eyes, nose and lips, and translate it to a set of points we can identify as such. This step is done by a trained algorithm developed by another researchers. We just select the points that are interesting for us, which are defined in the following image:<br><br>
		<img border="0" src="web_imagenes/fid_points.png" class="fotocentro"><br><br>
		After identifying these points (called fiducial points) in both the target image and the neutral one, the algorithm normalizes the distances between points in relation to the distance between the inner eye points. With the normalized distances, the next step is comparing both pictures in order to find AUs. This step ends when each AU has a value from 0 to 1 depending on how intense that AU is in the analysed picture. This process is shown on the flowchar below<br><br>
		<img border="0" src="web_imagenes/flujogramaweb.png" class="fotocentro"><br><br>
		The final step is running a classifier with the values of each AU. We tried different classifiers both linear and non-linear. They take the value of all the AUs and decide if the image shows happiness; after that, we compare it to the real value of happiness, provided by the selected database, and asign a ratio of success to each classifier.
		<br><br>
		Furthermore, we have developed a user-friendly program that executes all these steps with the pictures of your choosing. The whole algorithm and user interface can be found in our 
		<a href="https://github.com/JLBicho/VisionPorComputador/"> GitHub repository</a> 
		, open for everyone to download.
	</div>

	<div class="texto" align="justify">
		<h2 id="titulo" align="left">Results</h2>
		<i>Under construction</i></br>
		Measures of effectiveness and such.
	</div>

	<div class="texto" align="justify">
		<h2 id="titulo" align="right">App in real time</h2>
		<i>Under construction</i></br>
		We will try to embed the program here to work in real time.
	</div>
</div>

	<div id="redes">
		<a  href="https://twitter.com/IronMaiden" target="_blank">
			<img border="0" id="icon" src="web_imagenes/logo_twitter.png" width="50" height="50">
		</a>
	</br></br>
	<a  href="https://ironmaiden.com/" target="_blank">
		<img border="0" id="icon" src="web_imagenes/logo_fb.png" width="50" height="50">
	</a>
	</br></br>
	<a  href="https://www.instagram.com/ironmaiden/" target="_blank">
		<img border="0" id="icon" src="web_imagenes/logo_instagram.png" width="50" height="50">
	</a>
	</br></br>
	<img border="0" id="icon" src="web_imagenes/logo_mail.png" width="50" height="50" onclick="copiarMail()">
	<input type="text" value="ironmaiden@gmail.com" id="mail" hidden>
	</div>
</body>
</html>